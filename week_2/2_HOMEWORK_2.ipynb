{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugarghost/hanghae99_AI_PLUS_2/blob/main/2_HOMEWORK_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2주차] 심화과제: Multi-head Attention으로 감정 분석 모델 구현하기"
      ],
      "metadata": {
        "id": "MTueAaFZikC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1X7RM2du1zcr",
        "outputId": "457a042a-9e89-4245-b631-227b69a66654",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.11.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HOdhoBVA1zcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97420a03-0a4a-4a38-d236-d3faaba7338b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence # [my code] 추가\n",
        "from transformers import BertTokenizerFast\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['text'])\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-head attention(MHA) 구현\n",
        "  - Self-attention module을 MHA로 확장해주시면 됩니다. 여기서 MHA는 다음과 같이 구현합니다.\n",
        "    1. 기존의 $W_q, W_k, W_v$를 사용하여 $Q, K, V$를 생성합니다. 이 부분은 코드 수정이 필요 없습니다.\n",
        "    2. $Q, K, V \\in \\mathbb{R}^{S \\times D}$가 있을 때, 이를 $Q, K, V \\in \\mathbb{R}^{S \\times H \\times D’}$으로 reshape 해줍니다. 여기서 $H$는 `n_heads`라는 인자로 받아야 하고, $D$가 $H$로 나눠 떨어지는 값이여야 하는 제약 조건이 필요합니다. $D = H \\times D’$입니다.\n",
        "    3. $Q, K, V$를 $Q, K, V \\in \\mathbb{R}^{H \\times S \\times D’}$의 shape으로 transpose해줍니다.\n",
        "    4. $A = QK^T/\\sqrt{D'} \\in \\mathbb{R}^{H \\times S \\times S}$를 기존의 self-attention과 똑같이 계산합니다. 이 부분은 코드 수정이 필요 없습니다.\n",
        "    5. Mask를 더합니다. 기존과 $A$의 shape이 달라졌기 때문에 dimension을 어떻게 맞춰줘야할지 생각해줘야 합니다.\n",
        "    6. $\\hat{x} = \\textrm{Softmax}(A)V \\in \\mathbb{R}^{H \\times S \\times D'}$를 계산해주고 transpose와 reshape을 통해 $\\hat{x} \\in \\mathbb{R}^{S \\times D}$의 shape으로 다시 만들어줍니다.\n",
        "    7. 기존과 똑같이 $\\hat{x} = \\hat{x} W_o$를 곱해줘서 마무리 해줍니다. 이 또한 코드 수정이 필요 없습니다."
      ],
      "metadata": {
        "id": "0tClQdj1iyuG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MBlMVMZcRAxv"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, input_dim, d_model):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.wq = nn.Linear(input_dim, d_model)\n",
        "    self.wk = nn.Linear(input_dim, d_model)\n",
        "    self.wv = nn.Linear(input_dim, d_model)\n",
        "    self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
        "    score = torch.matmul(q, k.transpose(-1, -2)) # (B, S, D) * (B, D, S) = (B, S, S)\n",
        "    score = score / sqrt(self.d_model)\n",
        "\n",
        "    if mask is not None:\n",
        "      score = score + (mask * -1e9)\n",
        "\n",
        "    score = self.softmax(score)\n",
        "    result = torch.matmul(score, v)\n",
        "    result = self.dense(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# [MY CODE] 멀티헤드 어텐션\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, n_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_head = d_model // n_heads  # D' = D / H\n",
        "\n",
        "        # Q, K, V 각각 가중치 매트릭스 생성\n",
        "        self.W_q = nn.Linear(input_dim, d_model)\n",
        "        self.W_k = nn.Linear(input_dim, d_model)\n",
        "        self.W_v = nn.Linear(input_dim, d_model)\n",
        "        self.dense = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "\n",
        "        # Q, K, V 생성\n",
        "        Q = self.W_q(x)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "\n",
        "        score = torch.matmul(Q, K.transpose(-1, -2))\n",
        "        score = score / sqrt(self.d_head)\n",
        "\n",
        "        if mask is not None:\n",
        "          # [MY CODE] 마스크 차원 변경\n",
        "          mask = mask.unsqueeze(1)\n",
        "          score = score + (mask * -1e9)\n",
        "\n",
        "        score = self.softmax(score)\n",
        "        output = torch.matmul(score, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.dense(output)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer normalization, dropout, residual connection 구현\n",
        "  - 다시 `TransformerLayer` class로 돌아와서 과제를 진행하시면 됩니다.\n",
        "  - Attention module을 $MHA$, feed-forward layer를 $FFN$이라고 하겠습니다."
      ],
      "metadata": {
        "id": "Z84esEcLi6p3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VZHPCn9AS5Gp"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self, input_dim, d_model, n_heads, dff, dropout_param):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.d_model = d_model\n",
        "    self.dff = dff\n",
        "\n",
        "    self.mha = MultiHeadAttention(input_dim, d_model, n_heads) # [MY CODE] 멀티헤드 어텐션 추가\n",
        "    #self.sa = SelfAttention(input_dim, d_model)\n",
        "    self.ffn = nn.Sequential(\n",
        "      nn.Linear(d_model, dff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(dff, d_model)\n",
        "    )\n",
        "\n",
        "    # [MY CODE] Layer 추가 및 드롭아웃 적용\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(dropout_param)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "\n",
        "    # [MY CODE] Multi-Head Attention 적용\n",
        "    mha = self.mha(x, mask)\n",
        "    mha = self.dropout(mha)\n",
        "    x = self.layernorm1(mha + x)\n",
        "\n",
        "    ffn = self.ffn(x)\n",
        "    ffn = self.dropout(ffn)\n",
        "    x = self.layernorm2(ffn + x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf_jMQWDUR79",
        "outputId": "9dabedcc-8f35-4050-f05e-d6d5c838baf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 400, 256])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[None, ...]\n",
        "\n",
        "    return torch.FloatTensor(pos_encoding)\n",
        "\n",
        "\n",
        "max_len = 400\n",
        "print(positional_encoding(max_len, 256).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-layer 4-head Transformer\n",
        "  - 기존 실습에서 사용한 hyper-parameter들과 위에서 구현한 Transformer를 가지고 5-layer 4-head Transformer의 성능 결과를 report해주시면 됩니다."
      ],
      "metadata": {
        "id": "s3WXDx_YjB1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8MaiCGh8TsDH"
      },
      "outputs": [],
      "source": [
        "class TextClassifier(nn.Module):\n",
        "  # [MY CODE] 매개변수 조정\n",
        "  def __init__(self, vocab_size, d_model, n_layers, n_heads, dff, dropout_param = 0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.n_layers = n_layers\n",
        "    self.dff = dff\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = nn.parameter.Parameter(positional_encoding(max_len, d_model), requires_grad=False)\n",
        "    self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, n_heads, dff, dropout_param) for _ in range(n_layers)])\n",
        "    self.classification = nn.Linear(d_model, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mask = (x == tokenizer.pad_token_id)\n",
        "    mask = mask[:, None, :]\n",
        "    seq_len = x.shape[1]\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x = x * sqrt(self.d_model)\n",
        "    x = x + self.pos_encoding[:, :seq_len]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "\n",
        "    x = x[:, 0]\n",
        "    x = self.classification(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier(len(tokenizer), 32, 5, 4, 32, 0.1) # [MY CODE] 5-layer 4-head Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YHVVsWBPQmnv"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r88BALxO1zc1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt\n",
        "\n",
        "# [MY CODE] 측정 함수 추가\n",
        "def plot_acc(train_accs, test_accs, label1='train', label2='test'):\n",
        "  x = np.arange(len(train_accs))\n",
        "\n",
        "  plt.plot(x, train_accs, label=label1)\n",
        "  plt.plot(x, test_accs, label=label2)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "al_b56TYRILq"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "# [MY CODE] train 로직 분리\n",
        "def train(model, optimizer, trainloader, testloader, n_epochs):\n",
        "  train_acc_list = []\n",
        "  test_acc_list = []\n",
        "  for epoch in range(n_epochs):\n",
        "    total_loss = 0.\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "      preds = model(inputs)[..., 0]\n",
        "      loss = loss_fn(preds, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      train_acc = accuracy(model, train_loader)\n",
        "      test_acc = accuracy(model, test_loader)\n",
        "      train_acc_list.append(train_acc)\n",
        "      test_acc_list.append(test_acc)\n",
        "      print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n",
        "  # MY CODE 끔직하게도 LIST를 내다 던져버림\n",
        "  return train_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc_list, test_acc_list = train(model, optimizer, train_loader, test_loader, n_epochs)\n",
        "plot_acc(train_acc_list, test_acc_list) # MY CODE 망함\n",
        "# [LOG] 다시 돌릴 리소스가 모잘라서...기존 PRINT 내역으로 만족 학습은 안정적으로 된것같고 loss가 많이 줄은 상태라 test acc는 변동이 적음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UW8zhWD-VoB_",
        "outputId": "0cb56e83-9ae2-4b4e-b344-cba4c4faff74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 216.41581273078918\n",
            "=========> Train acc: 0.815 | Test acc: 0.786\n",
            "Epoch   1 | Train Loss: 146.84099520742893\n",
            "=========> Train acc: 0.873 | Test acc: 0.814\n",
            "Epoch   2 | Train Loss: 119.5985151603818\n",
            "=========> Train acc: 0.915 | Test acc: 0.835\n",
            "Epoch   3 | Train Loss: 95.06357414275408\n",
            "=========> Train acc: 0.942 | Test acc: 0.836\n",
            "Epoch   4 | Train Loss: 75.2876220792532\n",
            "=========> Train acc: 0.958 | Test acc: 0.834\n",
            "Epoch   5 | Train Loss: 55.99532072991133\n",
            "=========> Train acc: 0.977 | Test acc: 0.832\n",
            "Epoch   6 | Train Loss: 42.942104674875736\n",
            "=========> Train acc: 0.984 | Test acc: 0.827\n",
            "Epoch   7 | Train Loss: 31.157082250341773\n",
            "=========> Train acc: 0.987 | Test acc: 0.832\n",
            "Epoch   8 | Train Loss: 25.706993332132697\n",
            "=========> Train acc: 0.989 | Test acc: 0.824\n",
            "Epoch   9 | Train Loss: 22.0138824111782\n",
            "=========> Train acc: 0.988 | Test acc: 0.820\n",
            "Epoch  10 | Train Loss: 19.82174086244777\n",
            "=========> Train acc: 0.994 | Test acc: 0.828\n",
            "Epoch  11 | Train Loss: 16.57027206593193\n",
            "=========> Train acc: 0.994 | Test acc: 0.829\n",
            "Epoch  12 | Train Loss: 14.40194210736081\n",
            "=========> Train acc: 0.996 | Test acc: 0.828\n",
            "Epoch  13 | Train Loss: 13.799567785812542\n",
            "=========> Train acc: 0.996 | Test acc: 0.826\n",
            "Epoch  14 | Train Loss: 13.288637334946543\n",
            "=========> Train acc: 0.986 | Test acc: 0.816\n",
            "Epoch  15 | Train Loss: 12.95441692462191\n",
            "=========> Train acc: 0.995 | Test acc: 0.826\n",
            "Epoch  16 | Train Loss: 11.742997315945104\n",
            "=========> Train acc: 0.996 | Test acc: 0.828\n",
            "Epoch  17 | Train Loss: 11.555974917951971\n",
            "=========> Train acc: 0.996 | Test acc: 0.828\n",
            "Epoch  18 | Train Loss: 12.175457398523577\n",
            "=========> Train acc: 0.996 | Test acc: 0.828\n",
            "Epoch  19 | Train Loss: 10.716775225591846\n",
            "=========> Train acc: 0.995 | Test acc: 0.825\n",
            "Epoch  20 | Train Loss: 11.772582493256778\n",
            "=========> Train acc: 0.996 | Test acc: 0.828\n",
            "Epoch  21 | Train Loss: 10.46569542947691\n",
            "=========> Train acc: 0.993 | Test acc: 0.825\n",
            "Epoch  22 | Train Loss: 10.113245252985507\n",
            "=========> Train acc: 0.997 | Test acc: 0.826\n",
            "Epoch  23 | Train Loss: 9.691340578487143\n",
            "=========> Train acc: 0.994 | Test acc: 0.825\n",
            "Epoch  24 | Train Loss: 10.039043245487846\n",
            "=========> Train acc: 0.996 | Test acc: 0.827\n",
            "Epoch  25 | Train Loss: 9.700573388370685\n",
            "=========> Train acc: 0.995 | Test acc: 0.830\n",
            "Epoch  26 | Train Loss: 9.63121324556414\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  27 | Train Loss: 9.562772541190498\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  28 | Train Loss: 9.040177852846682\n",
            "=========> Train acc: 0.996 | Test acc: 0.827\n",
            "Epoch  29 | Train Loss: 10.605619528447278\n",
            "=========> Train acc: 0.997 | Test acc: 0.826\n",
            "Epoch  30 | Train Loss: 8.457846101024188\n",
            "=========> Train acc: 0.996 | Test acc: 0.823\n",
            "Epoch  31 | Train Loss: 8.303843633679207\n",
            "=========> Train acc: 0.996 | Test acc: 0.822\n",
            "Epoch  32 | Train Loss: 8.245518013718538\n",
            "=========> Train acc: 0.997 | Test acc: 0.826\n",
            "Epoch  33 | Train Loss: 8.380327815655619\n",
            "=========> Train acc: 0.995 | Test acc: 0.830\n",
            "Epoch  34 | Train Loss: 8.212614396470599\n",
            "=========> Train acc: 0.997 | Test acc: 0.823\n",
            "Epoch  35 | Train Loss: 8.630281814199407\n",
            "=========> Train acc: 0.996 | Test acc: 0.827\n",
            "Epoch  36 | Train Loss: 8.465662922826596\n",
            "=========> Train acc: 0.997 | Test acc: 0.826\n",
            "Epoch  37 | Train Loss: 8.00244636030402\n",
            "=========> Train acc: 0.997 | Test acc: 0.826\n",
            "Epoch  38 | Train Loss: 6.4015842821099795\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  39 | Train Loss: 9.033501361380331\n",
            "=========> Train acc: 0.995 | Test acc: 0.826\n",
            "Epoch  40 | Train Loss: 7.039796621073037\n",
            "=========> Train acc: 0.995 | Test acc: 0.825\n",
            "Epoch  41 | Train Loss: 7.711128348018974\n",
            "=========> Train acc: 0.997 | Test acc: 0.823\n",
            "Epoch  42 | Train Loss: 7.385987843212206\n",
            "=========> Train acc: 0.997 | Test acc: 0.824\n",
            "Epoch  43 | Train Loss: 6.335146389377769\n",
            "=========> Train acc: 0.996 | Test acc: 0.826\n",
            "Epoch  44 | Train Loss: 8.036266417184379\n",
            "=========> Train acc: 0.997 | Test acc: 0.830\n",
            "Epoch  45 | Train Loss: 6.728096090402687\n",
            "=========> Train acc: 0.990 | Test acc: 0.813\n",
            "Epoch  46 | Train Loss: 7.102522494911682\n",
            "=========> Train acc: 0.997 | Test acc: 0.825\n",
            "Epoch  47 | Train Loss: 6.477844080218347\n",
            "=========> Train acc: 0.998 | Test acc: 0.826\n",
            "Epoch  48 | Train Loss: 5.14218210359104\n",
            "=========> Train acc: 0.997 | Test acc: 0.825\n",
            "Epoch  49 | Train Loss: 5.311943179258378\n",
            "=========> Train acc: 0.997 | Test acc: 0.819\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'float' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9f81b11e349c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3e6cec40420c>\u001b[0m in \u001b[0;36mplot_acc\u001b[0;34m(train_accs, test_accs, label1, label2)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# [MY CODE] 측정 함수 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}